{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "583993c6-cd49-46fe-84e4-6f11c52661b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision.transforms\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "from PIL import Image, ImageOps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d877014e-93a0-4a61-8e84-fd12722e1c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"\\Train\"\n",
    "\n",
    "val_path = \"\\Validation\"\n",
    "\n",
    "test_path = \"\\Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e4fa20f4-730f-4c1d-8876-ce51e791eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kornia\n",
    "import torch.utils.data as data\n",
    "import glob\n",
    "import os\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import kornia\n",
    "from torchvision.transforms import RandomGrayscale, ColorJitter, RandomHorizontalFlip, Resize, Normalize\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class DataLoaderSegmentationAndColorisation(data.Dataset):\n",
    "    def __init__(self, folder_path, transform):\n",
    "        super(DataLoaderSegmentationAndColorisation, self).__init__()\n",
    "        \n",
    "        #print(os.getcwd() + \"\\Dataset\" + folder_path + \"\\Images\\*.png\")\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.img_files = glob.glob(os.getcwd() + \"\\Dataset\" + folder_path + \"\\Images\\*.png\")\n",
    "        self.mask_files = glob.glob(os.getcwd() + \"\\Dataset\" + folder_path + \"\\Labels\\*.png\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            mask_path = self.mask_files[index]\n",
    "            \n",
    "            data = Image.open(img_path) # PIL images\n",
    "            SEG = Image.open(mask_path)\n",
    "        \n",
    "            if self.transform is not None:\n",
    "                data = self.transform(data)\n",
    "                SEG = self.transform(SEG) # segmentation should probably only be resized\n",
    "\n",
    "            transform = transforms.Compose([ \n",
    "                transforms.PILToTensor() \n",
    "            ]) \n",
    "\n",
    "            img_lab = kornia.color.rgb_to_lab(transform(data))\n",
    "\n",
    "            L = (img_lab.permute(1, 2, 0))[:,:,0]\n",
    "            A = (img_lab.permute(1, 2, 0))[:,:,1]\n",
    "            B = (img_lab.permute(1, 2, 0))[:,:,2]\n",
    "\n",
    "            SEG = transform(SEG)\n",
    "            SEG = torch.squeeze(SEG)\n",
    "\n",
    "            input = torch.stack((L,SEG))\n",
    "            output = torch.stack((A,B))\n",
    "\n",
    "            \n",
    "            # data = [L and seg] label = [A, B]\n",
    "            input = input.to(torch.float32) \n",
    "            output = output.to(torch.float32)       \n",
    "            return input, output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "example_transform = Compose([Resize((128,128), antialias=True)])\n",
    "example_dataset = DataLoaderSegmentationAndColorisation(train_path,example_transform)\n",
    "\n",
    "input, output = example_dataset[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2147924b-22f5-479c-a696-fc9e76bc8652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 2\n",
    "#n_classes = 35 # Only for segmentation\n",
    "img_size = 128\n",
    "\n",
    "train_transforms = Compose([Resize((img_size,img_size), antialias=True)])\n",
    "\n",
    "example_transform = Compose([Resize((128,128), antialias=True)])\n",
    "example_dataset = DataLoaderSegmentationAndColorisation(train_path,example_transform)\n",
    "\n",
    "input, output = example_dataset[30]\n",
    "\n",
    "print(input.shape)\n",
    "print(output.shape)\n",
    "\n",
    "train_set = DataLoaderSegmentationAndColorisation(train_path, train_transforms)\n",
    "val_set = DataLoaderSegmentationAndColorisation(val_path, train_transforms)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "65dce0f0-2ab7-4f40-acb5-629d5be8e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.relu = nn.ReLU()\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c)\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs) \n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class build_unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.e1 = encoder_block(2, 64) # Maybe 2 since L and SEG\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.b = conv_block(512, 1024)\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64) \n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(64, 2, kernel_size=1, padding=0) # 2 since A and B\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        print(inputs.shape)\n",
    "        #inputs = inputs.permute(1,0,2,3)\n",
    "        inputs = torch.squeeze(inputs)\n",
    "        s1, p1 = self.e1(inputs)  \n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(d4)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6496006-69c4-45a6-80c4-e78d92e08c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every):\n",
    "    print(\"Starting training\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model, train_loss, train_acc = train_epoch(model,\n",
    "                                                   optimizer,\n",
    "                                                   loss_fn,\n",
    "                                                   train_loader,\n",
    "                                                   val_loader,\n",
    "                                                   device,\n",
    "                                                   print_every)\n",
    "        # Validate fucks up\n",
    "        val_loss, val_acc = validate(model, loss_fn, val_loader, device)\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "              f\"Train loss: {sum(train_loss)/len(train_loss):.3f}, \"\n",
    "              #f\"Train acc.: {sum(train_acc)/len(train_acc):.3f}, \"\n",
    "              f\"Val. loss: {val_loss:.3f}, \"\n",
    "              f\"Val. acc.: {val_acc:.3f}\")\n",
    "        train_losses.extend(train_loss)\n",
    "        train_accs.extend(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        # Save model to disk after each epoch\n",
    "        torch.save(model.state_dict(), 'segmentation_model.pth')\n",
    "        \n",
    "    return model, train_losses, train_accs, val_losses, val_accs\n",
    "\n",
    "def train_epoch(model, optimizer, loss_fn, train_loader, val_loader, device, print_every):\n",
    "    # Train:\n",
    "    model.train()\n",
    "    train_loss_batches, train_acc_batches = [], []\n",
    "    num_batches = len(train_loader)\n",
    "    for batch_index, (x, y) in enumerate(train_loader, 1):    \n",
    "        inputs, labels = x.to(device), y.to(device)\n",
    "        inputs = inputs[None, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        z = model.forward(inputs)\n",
    "        loss = loss_fn(z, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_batches.append(loss.item())\n",
    "\n",
    "\n",
    "        print(\"Thinking.... \", batch_index)\n",
    "        # If you want to print your progress more often than every epoch you can\n",
    "        # set `print_every` to the number of batches you want between every status update.\n",
    "        # Note that the print out will trigger a full validation on the full val. set => slows down training\n",
    "        if print_every is not None and batch_index % print_every == 0:\n",
    "            val_loss, val_acc = validate(model, loss_fn, val_loader, device)\n",
    "            model.train()\n",
    "            print(f\"\\tBatch {batch_index}/{num_batches}: \"\n",
    "                  f\"\\tTrain loss: {sum(train_loss_batches[-print_every:])/print_every:.3f}, \"\n",
    "                  f\"\\tTrain acc.: {sum(train_acc_batches[-print_every:])/print_every:.3f}, \"\n",
    "                  f\"\\tVal. loss: {val_loss:.3f}, \"\n",
    "                  f\"\\tVal. acc.: {val_acc:.3f}\")\n",
    "\n",
    "    return model, train_loss_batches, train_acc_batches\n",
    "\n",
    "def validate(model, loss_fn, val_loader, device):\n",
    "    val_loss_cum = 0\n",
    "    val_acc_cum = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (x, y) in enumerate(val_loader, 1):\n",
    "            inputs, labels = x.to(device), y.to(device)\n",
    "            inputs = inputs[None, :]\n",
    "            \n",
    "            z = model.forward(inputs)\n",
    "            z = torch.round(z)\n",
    "            batch_loss = loss_fn(z, labels.long()) \n",
    "            val_loss_cum += batch_loss.item()      \n",
    "            \n",
    "            # Visual test\n",
    "            if batch_index == 1:\n",
    "                z, _ = torch.max(z[0,:,:,:],dim=0)\n",
    "                f, axarr = plt.subplots(1,3)\n",
    "                axarr[0].imshow(inputs[0,0,:,:],'grey')\n",
    "                \n",
    "                # With static cmap\n",
    "                #print(z[:,:])\n",
    "                #print(labels[0,:,:].float())\n",
    "                axarr[1].imshow(z[:,:],cmap)\n",
    "                axarr[2].imshow(labels[0,:,:].float(),cmap)\n",
    "\n",
    "                # Without static cmap\n",
    "                #axarr[1].imshow(z[0,:,:])\n",
    "                #axarr[2].imshow(labels.float()[0,:,:])\n",
    "                \n",
    "                plt.show()\n",
    "                # Save the model each time we plot (REMOVE LATER)\n",
    "                torch.save(model.state_dict(), 'segmentation_model.pth')\n",
    "\n",
    "    return val_loss_cum/len(val_loader), val_acc_cum/len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05b5a346-11e5-4d02-99d2-59f2801da14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new model\n",
      "Starting training\n",
      "torch.Size([1, 2, 2, 128, 128])\n",
      "Thinking....  1\n",
      "torch.Size([1, 2, 2, 128, 128])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (2, 128, 128) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     22\u001b[0m print_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 24\u001b[0m Segmentaion_and_color_model, Segmentaion_and_color_train_losses, Segmentaion_and_color_train_accs, Segmentaion_and_color_val_losses, Segmentaion_and_color_val_accs\u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSegmentaion_and_color_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every)\u001b[0m\n\u001b[0;32m      6\u001b[0m train_losses, train_accs, val_losses, val_accs \u001b[38;5;241m=\u001b[39m [], [], [], []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m     model, train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Validate fucks up\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(model, loss_fn, val_loader, device)\n",
      "Cell \u001b[1;32mIn[12], line 55\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, optimizer, loss_fn, train_loader, val_loader, device, print_every)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# If you want to print your progress more often than every epoch you can\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# set `print_every` to the number of batches you want between every status update.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Note that the print out will trigger a full validation on the full val. set => slows down training\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_every \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m batch_index \u001b[38;5;241m%\u001b[39m print_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 55\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(train_loss_batches[\u001b[38;5;241m-\u001b[39mprint_every:])\u001b[38;5;241m/\u001b[39mprint_every\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTrain acc.: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(train_acc_batches[\u001b[38;5;241m-\u001b[39mprint_every:])\u001b[38;5;241m/\u001b[39mprint_every\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mVal. loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mVal. acc.: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 83\u001b[0m, in \u001b[0;36mvalidate\u001b[1;34m(model, loss_fn, val_loader, device)\u001b[0m\n\u001b[0;32m     81\u001b[0m z, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(z[\u001b[38;5;241m0\u001b[39m,:,:,:],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     82\u001b[0m f, axarr \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 83\u001b[0m \u001b[43maxarr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrey\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# With static cmap\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m#print(z[:,:])\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m#print(labels[0,:,:].float())\u001b[39;00m\n\u001b[0;32m     88\u001b[0m axarr[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(z[:,:],cmap)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dmlproject\\lib\\site-packages\\matplotlib\\__init__.py:1465\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(ax, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1467\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1468\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1469\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dmlproject\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5751\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5749\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m-> 5751\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5752\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5754\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dmlproject\\lib\\site-packages\\matplotlib\\image.py:723\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    722\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[1;32m--> 723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dmlproject\\lib\\site-packages\\matplotlib\\image.py:693\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[1;34m(A)\u001b[0m\n\u001b[0;32m    691\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[1;32m--> 693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (2, 128, 128) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_from_disk = False\n",
    "\n",
    "if train_from_disk:  \n",
    "    # Read model from disk\n",
    "    print(\"reding model from disk\")\n",
    "    pretrained = torch.load('segmentation_and_color_model.pth', map_location=lambda storage, loc: storage)\n",
    "    Segmentaion_and_color_model = build_unet()\n",
    "    Segmentaion_and_color_model.load_state_dict(torch.load('segmentation_and_color_model.pth'))\n",
    "else:\n",
    "    # Create new model\n",
    "    print(\"creating new model\")\n",
    "    Segmentaion_and_color_model = build_unet()\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(Segmentaion_and_color_model.parameters(), lr =0.001)\n",
    "loss_fn = nn.MSELoss() # kass som faen men int nu\n",
    "#loss_fn = nn.CrossEntropyLoss() # mindre kass\n",
    "train_loader = train_dataloader \n",
    "val_loader = val_dataloader\n",
    "num_epochs = 10\n",
    "print_every = 1\n",
    "\n",
    "Segmentaion_and_color_model, Segmentaion_and_color_train_losses, Segmentaion_and_color_train_accs, Segmentaion_and_color_val_losses, Segmentaion_and_color_val_accs= training_loop(Segmentaion_and_color_model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d20dd-3fe2-4ac7-a177-b5d7a3bc960d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adb6fae-8a3c-4c8c-87d1-94884e2db82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3391bbb2-77f0-43e6-ad36-6ce252e39389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
