{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc4ca4a3",
   "metadata": {},
   "source": [
    "#  Project group 30 \n",
    "## Image colorization by combining semantic segmentation and autoencoding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d77819",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8775d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn, optim\n",
    "from torchvision.transforms import Compose, RandomGrayscale, ColorJitter, RandomHorizontalFlip, Resize, Normalize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "from PIL import Image, ImageOps \n",
    "\n",
    "#!pip install kornia\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import kornia\n",
    "\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca9f977",
   "metadata": {},
   "source": [
    "Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"\\Train\"\n",
    "\n",
    "val_path = \"\\Validation\"\n",
    "\n",
    "test_path = \"\\Test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7aff7",
   "metadata": {},
   "source": [
    "Training loop for autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f700fc36-67fc-4a1c-aec0-4c461af417b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_autoencoder(model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every):\n",
    "    \n",
    "    print(\"Starting training\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses, val_losses = [], []\n",
    "           \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model, train_loss = train_epoch_autoencoder(model,\n",
    "                                                   optimizer,\n",
    "                                                   loss_fn,\n",
    "                                                   train_loader,\n",
    "                                                   val_loader,\n",
    "                                                   device,\n",
    "                                                   print_every)\n",
    "        # Validate fucks up\n",
    "        val_loss = validate_autoencoder(model, loss_fn, val_loader, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "              f\"Train loss: {sum(train_loss)/len(train_loss):.3f}, \"\n",
    "              f\"Val. loss: {val_loss:.3f}, \")\n",
    "        train_losses.extend(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        file_name = 'loss_autoencoder.txt';\n",
    "        with open(file_name, \"a\") as file:\n",
    "            file.write(f\"Train loss epoch: {sum(train_loss_batches[-print_every:])/print_every:.3f}, Val. loss epoch: {val_loss:.3f}\\n\")\n",
    "                       \n",
    "        # Save model to disk after each epoch\n",
    "        torch.save(model.state_dict(), 'autoencoder_model.pth')\n",
    "        \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "def train_epoch_autoencoder(model, optimizer, loss_fn, train_loader, val_loader, device, print_every):\n",
    "    # Train:\n",
    "    model.train()\n",
    "    train_loss_batches = []\n",
    "    num_batches = len(train_loader)\n",
    "    for batch_index, (x, y) in enumerate(train_loader, 1):    \n",
    "        inputs, labels = x.to(device), y.to(device)\n",
    "        inputs = inputs[None, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        z = model.forward(inputs)\n",
    "        z = torch.squeeze(z)\n",
    "        #z = torch.round((z+1)*16) # 32 classes\n",
    "        loss = loss_fn(z, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_batches.append(loss.item())\n",
    "\n",
    "        print(f\"\\tBatch {batch_index}/{num_batches}\")\n",
    "        # If you want to print your progress more often than every epoch you can\n",
    "        # set `print_every` to the number of batches you want between every status update.\n",
    "        # Note that the print out will trigger a full validation on the full val. set => slows down training\n",
    "        if print_every is not None and batch_index % print_every == 0:\n",
    "            val_loss = validate_autoencoder(model, loss_fn, val_loader, device)\n",
    "            model.train()\n",
    "            print(f\"\\tTrain loss: {sum(train_loss_batches[-print_every:])/print_every:.3f}, \"\n",
    "                  f\"\\tVal. loss: {val_loss:.3f}, \")\n",
    "\n",
    "            file_name = 'loss_autoencoder.txt';\n",
    "            with open(file_name, \"a\") as file:\n",
    "                file.write(f\"Train loss batch: {sum(train_loss_batches[-print_every:])/print_every:.3f}, Val. loss batch: {val_loss:.3f}\\n\")\n",
    "\n",
    "    return model, train_loss_batches\n",
    "\n",
    "def validate_autoencoder(model, loss_fn, val_loader, device):\n",
    "    val_loss_cum = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (x, y) in enumerate(val_loader, 1):\n",
    "            inputs, labels = x.to(device), y.to(device)\n",
    "            inputs = inputs[None, :]\n",
    "            \n",
    "            z_ = model.forward(inputs)\n",
    "            z = torch.squeeze(z_)\n",
    "            \n",
    "            batch_loss = loss_fn(z, labels.float())\n",
    "            val_loss_cum += batch_loss.item()      \n",
    "            \n",
    "            # Visual test\n",
    "            if batch_index == 1:\n",
    "                f, axarr = plt.subplots(1,4)\n",
    "                \n",
    "                # With static cmap\n",
    "                L = inputs[0,0,:,:].to(device)\n",
    "                A = z[0,0,:,:].to(device)\n",
    "                B = z[0,1,:,:].to(device)\n",
    "                \n",
    "                Lfill = 50*torch.ones(L.shape).to(device)\n",
    "                Zfill = torch.zeros(L.shape).to(device)\n",
    "                \n",
    "                A_vis = kornia.color.lab_to_rgb(torch.stack((Lfill, A, Zfill),0)).permute(1, 2, 0)\n",
    "                B_vis = kornia.color.lab_to_rgb(torch.stack((Lfill, Zfill, B),0)).permute(1, 2, 0)\n",
    "                full_vis = kornia.color.lab_to_rgb(torch.stack((L, A, B),0)).permute(1, 2, 0)\n",
    "                \n",
    "                axarr[0].imshow(L,'grey')\n",
    "                axarr[1].imshow(A_vis)\n",
    "                axarr[2].imshow(B_vis)\n",
    "                axarr[3].imshow(full_vis)\n",
    "                \n",
    "                plt.show()\n",
    "                # Save the model each time we plot (REMOVE LATER)\n",
    "                torch.save(model.state_dict(), 'autoencoder_model.pth')\n",
    "                \n",
    "    return val_loss_cum/len(val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a2bd1d-3bd1-448d-b167-2d7167b99b8c",
   "metadata": {},
   "source": [
    "Dataloader construction for the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderAutoEncoder(data.Dataset):\n",
    "    def __init__(self, folder_path, transform):\n",
    "        super().__init__()\n",
    "        \n",
    "        #print(os.getcwd() + \"\\Dataset\" + folder_path + \"\\Images\\*.png\")\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.img_files = glob.glob(os.getcwd() + \"\\Dataset\" + folder_path + \"\\Images\\*.png\")\n",
    "        self.mask_files = glob.glob(os.getcwd() + \"\\Dataset\" + folder_path + \"\\Labels\\*.png\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            img_rgb = Image.open(img_path)\n",
    "\n",
    "            # Perform transforms, if any.\n",
    "            if self.transform is not None:\n",
    "                label = self.transform(img_rgb)\n",
    "            \n",
    "            label = np.array(label)\n",
    "            np.moveaxis(label, 0, -1).shape\n",
    "            label = torch.from_numpy(label).float()\n",
    "            label = label/255\n",
    "            label = kornia.color.rgb_to_lab(label.permute(2, 0, 1))\n",
    "            data = label[0, :, :]\n",
    "            label = label[1:, :, :]\n",
    "     \n",
    "            return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fdfb3d-9bd9-401c-8d27-cfdda2460482",
   "metadata": {},
   "source": [
    "Autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6dcafb-2d36-464e-9d30-af81dddffbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.relu = nn.ReLU()\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c)\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs) \n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "        \n",
    "class build_unet_autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.e1 = encoder_block(1, 64)\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.b = conv_block(512, 1024)\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64) \n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(64, 2, kernel_size=1, padding=0)\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        inputs = inputs.permute(1,0,2,3)\n",
    "        s1, p1 = self.e1(inputs)  \n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(d4)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a78861-89d0-4cf6-b5bd-b84bbed2300d",
   "metadata": {},
   "source": [
    "DataLoader autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a229e-83dd-4244-8a27-b1a563be8a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 256\n",
    "train_transforms = Compose([Resize((d,d), antialias=True)])\n",
    "\n",
    "train_set_autoencoder = DataLoaderAutoEncoder(train_path, train_transforms)\n",
    "val_set_autoencoder = DataLoaderAutoEncoder(val_path, train_transforms)\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader_autoencoder = DataLoader(train_set_autoencoder, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader_autoencoder = DataLoader(val_set_autoencoder, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7658d69c-7a93-4924-8d6a-284f6c5e656e",
   "metadata": {},
   "source": [
    "Visualization test for autoencoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff62ec-bac2-4beb-bdb1-ef870d5d2378",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_transform = Compose([Resize((d,d), antialias=True)])\n",
    "example_dataset = DataLoaderAutoEncoder(train_path,example_transform)\n",
    "\n",
    "img, label = example_dataset[30]\n",
    "L = img\n",
    "A = (label.permute(1,2,0))[:,:,0]\n",
    "B = (label.permute(1,2,0))[:,:,1]\n",
    "\n",
    "Lfill = 50*torch.ones(L.shape)\n",
    "Zfill = torch.zeros(L.shape)\n",
    "\n",
    "A_vis = kornia.color.lab_to_rgb(torch.stack((Lfill, A, Zfill),0)).permute(1, 2, 0)\n",
    "B_vis = kornia.color.lab_to_rgb(torch.stack((Lfill, Zfill, B),0)).permute(1, 2, 0)\n",
    "\n",
    "full_vis = kornia.color.lab_to_rgb(torch.stack((L, A, B),0)).permute(1, 2, 0)\n",
    "\n",
    "plt.figure(0)\n",
    "plt.imshow(img, 'grey')\n",
    "plt.figure(1)\n",
    "plt.imshow(A_vis)\n",
    "plt.figure(2)\n",
    "plt.imshow(B_vis)\n",
    "plt.figure(3)\n",
    "plt.imshow(full_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe530b7-aae0-49f9-badb-e5c0344153ea",
   "metadata": {},
   "source": [
    "Training of the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9200554c-995f-45b0-890e-71cdc5adeeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False if you want to create a new file saving to and True if you want to continue training on last file saved\n",
    "train_from_disk_autoencoder = False\n",
    "\n",
    "if train_from_disk_autoencoder:  \n",
    "    # Read model from disk\n",
    "    print(\"Reading model from disk\")\n",
    "    pretrained = torch.load('autoencoder_model.pth', map_location=lambda storage, loc: storage)\n",
    "    autoencoder_model = build_unet_autoencoder()\n",
    "    autoencoder_model.load_state_dict(torch.load('autoencoder_model.pth'))\n",
    "    print(\"Continue writing to loss textfile\")\n",
    "    file_name = 'loss_autoencoder.txt';\n",
    "    with open(file_name, \"a\") as file:\n",
    "        file.write('Continue:\\n') \n",
    "else:\n",
    "    # Create new model\n",
    "    print(\"Creating new model\")\n",
    "    autoencoder_model = build_unet_autoencoder()\n",
    "    file_name = 'loss_autoencoder.txt';\n",
    "    print(\"Starting over writing to loss textfile\")\n",
    "    with open(file_name, \"w\") as file:\n",
    "        file.write('Losses:\\n')\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder_model.parameters(), lr =0.001)\n",
    "train_loader = train_dataloader_autoencoder\n",
    "val_loader = val_dataloader_autoencoder\n",
    "num_epochs = 10\n",
    "print_every = 1\n",
    "\n",
    "autoencoder_model, autoencoder_train_losses, autoencoder_val_losses= training_loop_autoencoder(autoencoder_model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
