{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc4ca4a3",
   "metadata": {},
   "source": [
    "#  Project group 30 \n",
    "## Image colorization by combining semantic segmentation and autoencoding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d77819",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8775d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn, optim\n",
    "from torchvision.transforms import Compose, RandomGrayscale, ColorJitter, RandomHorizontalFlip, Resize, Normalize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "from PIL import Image, ImageOps \n",
    "\n",
    "#!pip install kornia\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import kornia\n",
    "\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca9f977",
   "metadata": {},
   "source": [
    "Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"\\Train\"\n",
    "\n",
    "val_path = \"\\Validation\"\n",
    "\n",
    "test_path = \"\\Test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d82503",
   "metadata": {},
   "source": [
    "Dataloader construction for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b843e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import glob\n",
    "import os\n",
    "\n",
    "class DataLoaderSegmentation(data.Dataset):\n",
    "    def __init__(self, folder_path, transform):\n",
    "        super(DataLoaderSegmentation, self).__init__()\n",
    "        \n",
    "        #print(os.getcwd() + \"\\Dataset\" + folder_path + \"\\Images\\*.png\")\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.img_files = glob.glob(os.getcwd() + \"\\Dataset\" + folder_path + \"\\Images\\*.png\")\n",
    "        self.mask_files = glob.glob(os.getcwd() + \"\\Dataset\" + folder_path + \"\\Labels\\*.png\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            mask_path = self.mask_files[index]\n",
    "            \n",
    "            data = Image.open(img_path) # PIL images\n",
    "            data = ImageOps.grayscale(data) \n",
    "        \n",
    "            label = Image.open(mask_path) \n",
    "            \n",
    "            # Perform transforms, if any.\n",
    "            if self.transform is not None:\n",
    "                data = self.transform(data)\n",
    "                label = self.transform(label)\n",
    "            \n",
    "            data = np.array(data)\n",
    "            np.moveaxis(data, 0, -1).shape\n",
    "            \n",
    "            label = np.array(label)\n",
    "            np.moveaxis(label, 0, -1).shape\n",
    "\n",
    "            # GÃ–R OM TILL GREY SCALE\n",
    "            \n",
    "            return torch.from_numpy(data).float(), torch.from_numpy(label).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d9ea6",
   "metadata": {},
   "source": [
    "Visualize test image for the dataloader of the segmentation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a0817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomGrayscale, ColorJitter, RandomHorizontalFlip, Resize, Normalize\n",
    "example_transform = Compose([Resize((128,128), antialias=True)])\n",
    "example_dataset = DataLoaderSegmentation(train_path,example_transform)\n",
    "\n",
    "img, label = example_dataset[30]\n",
    "\n",
    "plt.figure(0)\n",
    "plt.imshow(img/torch.max(img))\n",
    "plt.figure(1)\n",
    "plt.imshow(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c65c5da",
   "metadata": {},
   "source": [
    "Perform transforms on the images maybe??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384a9d24",
   "metadata": {},
   "source": [
    "Create dataloaders for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e62ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example_transform = Compose([Resize((128,128), antialias=True)])\n",
    "#example_dataset = DataLoaderSegmentation(train_path,example_transform)\n",
    "batch_size = 16\n",
    "n_classes = 35 # Crashes if too low idk why \n",
    "img_size = 128\n",
    "\n",
    "train_transforms = Compose([Resize((img_size,img_size), antialias=True)])\n",
    "\n",
    "train_set = DataLoaderSegmentation(train_path, train_transforms)\n",
    "val_set = DataLoaderSegmentation(val_path, train_transforms)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d299fb72",
   "metadata": {},
   "source": [
    "Build the model class for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source\n",
    "# https://medium.com/analytics-vidhya/unet-implementation-in-pytorch-idiot-developer-da40d955f201\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.relu = nn.ReLU()\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c)\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs) \n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class build_unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.e1 = encoder_block(1, 64)\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.b = conv_block(512, 1024)\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64) \n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(64, n_classes, kernel_size=1, padding=0) # 32 or 31 channels\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        inputs = inputs.permute(1,0,2,3)\n",
    "        s1, p1 = self.e1(inputs)  \n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(d4)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d616123",
   "metadata": {},
   "source": [
    "Build the training loop for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752813ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom colormap\n",
    "#cmap = colors.ListedColormap(['k','b','y','g','r','darkorange','forestgreen','lime','seagreen','royalblue','navy','indigo','teal','gold','tan','aqua','coral','olive','purple','maroon','turquoise','lightsalmon','pink','plum','khaki','lawngreen','peru','lightgray','wheat','sienna','fuchsia','thistle'])\n",
    "\n",
    "def get_cmap(n, name='hsv'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)\n",
    "\n",
    "cmap = get_cmap(n_classes)\n",
    "\n",
    "def training_loop(model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every):\n",
    "    print(\"Starting training\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model, train_loss = train_epoch(model,\n",
    "                                       optimizer,\n",
    "                                       loss_fn,\n",
    "                                       train_loader,\n",
    "                                       val_loader,\n",
    "                                       device,\n",
    "                                       print_every)\n",
    "        \n",
    "        # Validate fucks up\n",
    "        val_loss = validate(model, loss_fn, val_loader, device)\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "              f\"Train loss: {sum(train_loss)/len(train_loss):.3f}, \"\n",
    "              f\"Val. loss: {val_loss:.3f}, \")\n",
    "        train_losses.extend(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        file_name = 'loss_seg.txt';\n",
    "        with open(file_name, \"a\") as file:\n",
    "            file.write(f\"Train loss epoch: {sum(train_loss)/len(train_loss):.3f}, Val. loss epoch: {val_loss:.3f}\\n\")\n",
    "\n",
    "        # Save model to disk after each epoch\n",
    "        torch.save(model.state_dict(), 'segmentation_model.pth')\n",
    "        \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "def train_epoch(model, optimizer, loss_fn, train_loader, val_loader, device, print_every):\n",
    "    # Train:\n",
    "    model.train()\n",
    "    train_loss_batches = []\n",
    "    num_batches = len(train_loader)\n",
    "    for batch_index, (x, y) in enumerate(train_loader, 1):    \n",
    "        inputs, labels = x.to(device), y.to(device)\n",
    "        inputs = inputs[None, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        z = model.forward(inputs)\n",
    "        #z = z.permute(0, 2, 3, 1)  # Change the channel dimension to the last\n",
    "        #z = z.permute(0, 3, 1, 2) #[Batch, class, img, img]\n",
    "\n",
    "        #print(\"label size: \", labels.shape) #[batch, img, img]\n",
    "        #labels = labels.view(-1)  # Reshape to (N*H*W,)\n",
    "        \n",
    "        \n",
    "        #print(\"before loss_fn\")\n",
    "        #print(\"z.shape: \", z.shape)\n",
    "        #print(\"labels.shape: \", labels.shape)\n",
    "        loss = loss_fn(z, labels.long())\n",
    "        #loss = loss_fn(z, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_batches.append(loss.item())\n",
    "\n",
    "        print(f\"\\tBatch {batch_index}/{num_batches}\")\n",
    "        # If you want to print your progress more often than every epoch you can\n",
    "        # set `print_every` to the number of batches you want between every status update.\n",
    "        # Note that the print out will trigger a full validation on the full val. set => slows down training\n",
    "        if print_every is not None and batch_index % print_every == 0:\n",
    "            val_loss = validate(model, loss_fn, val_loader, device)\n",
    "            model.train()\n",
    "            print(f\"\\tTrain loss: {sum(train_loss_batches[-print_every:])/print_every:.3f}, \"\n",
    "                  f\"\\tVal. loss: {val_loss:.3f}, \")\n",
    "\n",
    "            file_name = 'loss_seg.txt';\n",
    "            with open(file_name, \"a\") as file:\n",
    "                file.write(f\"Train loss batch: {sum(train_loss_batches[-print_every:])/print_every:.3f}, Val. loss batch: {val_loss:.3f}\\n\")\n",
    "\n",
    "    return model, train_loss_batches\n",
    "\n",
    "def validate(model, loss_fn, val_loader, device):\n",
    "    val_loss_cum = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (x, y) in enumerate(val_loader, 1):\n",
    "            inputs, labels = x.to(device), y.to(device)\n",
    "            inputs = inputs[None, :]\n",
    "            \n",
    "            z = model.forward(inputs)\n",
    "            z = torch.round(z)\n",
    "            batch_loss = loss_fn(z, labels.long()) \n",
    "            val_loss_cum += batch_loss.item()      \n",
    "            \n",
    "            # Visual test\n",
    "            if batch_index == 1:\n",
    "                z, _ = torch.max(z[0,:,:,:],dim=0)\n",
    "                f, axarr = plt.subplots(1,3)\n",
    "                axarr[0].imshow(inputs[0,0,:,:],'grey')\n",
    "                \n",
    "                # With static cmap\n",
    "                #print(z[:,:])\n",
    "                #print(labels[0,:,:].float())\n",
    "                axarr[1].imshow(z[:,:],cmap)\n",
    "                axarr[2].imshow(labels[0,:,:].float(),cmap)\n",
    "\n",
    "                # Without static cmap\n",
    "                #axarr[1].imshow(z[0,:,:])\n",
    "                #axarr[2].imshow(labels.float()[0,:,:])\n",
    "                \n",
    "                plt.show()\n",
    "                # Save the model each time we plot (REMOVE LATER)\n",
    "                torch.save(model.state_dict(), 'segmentation_model.pth')\n",
    "\n",
    "    return val_loss_cum/len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b935ed6",
   "metadata": {},
   "source": [
    "Training the segmentation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd20626",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_from_disk = True\n",
    "\n",
    "if train_from_disk:  \n",
    "    # Read model from disk\n",
    "    print(\"Reading model from disk\")\n",
    "    pretrained = torch.load('segmentation_model.pth', map_location=lambda storage, loc: storage)\n",
    "    Segmentaion_model = build_unet()\n",
    "    Segmentaion_model.load_state_dict(torch.load('segmentation_model.pth'))\n",
    "    file_name = 'loss_seg.txt'\n",
    "    print(\"Continue writing to loss textfile\")\n",
    "    with open(file_name, \"a\") as file:\n",
    "        file.write('Continue:\\n') \n",
    "else:\n",
    "    # Create new model\n",
    "    print(\"Creating new model\")\n",
    "    Segmentaion_model = build_unet()\n",
    "    file_name = 'loss_seg.txt'\n",
    "    print(\"Starting over writing to loss textfile\")\n",
    "    with open(file_name, \"w\") as file:\n",
    "        file.write('Losses:\\n')\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(Segmentaion_model.parameters(), lr =0.001)\n",
    "#loss_fn = nn.MSELoss() # kass som faen\n",
    "loss_fn = nn.CrossEntropyLoss() # mindre kass\n",
    "train_loader = train_dataloader \n",
    "val_loader = val_dataloader\n",
    "num_epochs = 10\n",
    "print_every = 25\n",
    "\n",
    "Segmentaion_model, Segmentaion_train_losses, Segmentaion_val_losses = training_loop(Segmentaion_model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
