{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de351ea-a1d5-4448-8641-695995c66b6d",
   "metadata": {},
   "source": [
    "#  Project group 30 \n",
    "## Image colorization by combining semantic segmentation and autoencoding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3f1426-06f8-4ea2-a7ce-cfc68dc0b4b4",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583993c6-cd49-46fe-84e4-6f11c52661b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor, RandomGrayscale, ColorJitter, RandomHorizontalFlip, Resize, Normalize\n",
    "from torch import nn, optim\n",
    "import torchvision.transforms\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# !pip install kornia\n",
    "import kornia\n",
    "from PIL import Image, ImageOps \n",
    "\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aad2f22-c495-4504-b5d3-d6afcf7a98a0",
   "metadata": {},
   "source": [
    "Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877014e-93a0-4a61-8e84-fd12722e1c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"\\Train\"\n",
    "\n",
    "val_path = \"\\Validation\"\n",
    "\n",
    "test_path = \"\\Test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9a5e45-3b87-4b54-8eff-3ba7c26f1725",
   "metadata": {},
   "source": [
    "Class for creating dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa20f4-730f-4c1d-8876-ce51e791eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderSegmentationAndColorisation(data.Dataset):\n",
    "    def __init__(self, folder_path, transform):\n",
    "        super(DataLoaderSegmentationAndColorisation, self).__init__()\n",
    "        \n",
    "        #print(os.getcwd() + \"\\Dataset\" + folder_path + \"\\Images\\*.png\")\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.img_files = glob.glob(os.getcwd() + \"\\Dataset\" + folder_path + \"\\Images\\*.png\")\n",
    "        self.mask_files = glob.glob(os.getcwd() + \"\\Dataset\" + folder_path + \"\\Labels\\*.png\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            mask_path = self.mask_files[index]\n",
    "            \n",
    "            data = Image.open(img_path) # PIL images\n",
    "            SEG = Image.open(mask_path)\n",
    "        \n",
    "            if self.transform is not None:\n",
    "                data = self.transform(data)\n",
    "                SEG = self.transform(SEG) # segmentation should probably only be resized\n",
    "\n",
    "            # SOMETHING FISHY HERE\n",
    "            #transform = transforms.Compose([ \n",
    "            #    transforms.PILToTensor() \n",
    "            #]) \n",
    "\n",
    "            #img_lab = kornia.color.rgb_to_lab(transform(data))\n",
    "            SEG = np.array(SEG)\n",
    "            np.moveaxis(SEG, 0, -1).shape\n",
    "            SEG = torch.from_numpy(SEG).float()\n",
    "        \n",
    "            img_rgb = np.array(data)\n",
    "            np.moveaxis(img_rgb, 0, -1).shape\n",
    "            img_rgb = torch.from_numpy(img_rgb).float()\n",
    "            img_rgb = img_rgb/255\n",
    "            img_lab = kornia.color.rgb_to_lab(img_rgb.permute(2, 0, 1))\n",
    "\n",
    "            L = (img_lab.permute(1, 2, 0))[:,:,0]\n",
    "            A = (img_lab.permute(1, 2, 0))[:,:,1]\n",
    "            B = (img_lab.permute(1, 2, 0))[:,:,2]\n",
    "        \n",
    "            input = torch.stack((L,SEG))\n",
    "            output = torch.stack((A,B))\n",
    "            \n",
    "            # data = [L, SEG] label = [A, B]\n",
    "            input = input.to(torch.float32) \n",
    "            output = output.to(torch.float32)       \n",
    "            return input, output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "example_transform = Compose([Resize((128,128), antialias=True)])\n",
    "example_dataset = DataLoaderSegmentationAndColorisation(train_path,example_transform)\n",
    "\n",
    "input, output = example_dataset[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850cd810-0a2d-441b-960d-b3b41e7ca09e",
   "metadata": {},
   "source": [
    "Creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147924b-22f5-479c-a696-fc9e76bc8652",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "img_size = 256 \n",
    "\n",
    "train_transforms = Compose([Resize((img_size,img_size), antialias=True)])\n",
    "\n",
    "example_transform = Compose([Resize((img_size,img_size), antialias=True)])\n",
    "example_dataset = DataLoaderSegmentationAndColorisation(train_path,example_transform)\n",
    "\n",
    "input, output = example_dataset[30]\n",
    "\n",
    "#print(input.shape)\n",
    "#print(output.shape)\n",
    "\n",
    "train_set = DataLoaderSegmentationAndColorisation(train_path, train_transforms)\n",
    "val_set = DataLoaderSegmentationAndColorisation(val_path, train_transforms)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f642be-ea2f-4fa9-80cb-a3f0d812b140",
   "metadata": {},
   "source": [
    "Defining network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dce0f0-2ab7-4f40-acb5-629d5be8e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.relu = nn.ReLU()\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c)\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs) \n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class build_unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.e1 = encoder_block(2, 64) # Maybe 2 since L and SEG\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.b = conv_block(512, 1024)\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64) \n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(64, 2, kernel_size=1, padding=0) # 2 since A and B\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        #print(inputs.shape)\n",
    "        #inputs = inputs.permute(1,0,2,3)\n",
    "        inputs = torch.squeeze(inputs)\n",
    "        s1, p1 = self.e1(inputs)  \n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(d4)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0986c233-6bbb-4488-9711-932c82712f36",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6496006-69c4-45a6-80c4-e78d92e08c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_cmap(n, name='hsv'):\n",
    "#    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "#    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "#    return plt.cm.get_cmap(name, n)\n",
    "\n",
    "#cmap = get_cmap(n_classes)\n",
    "\n",
    "def training_loop(model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every):\n",
    "    print(\"Starting training\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model, train_loss = train_epoch(model,\n",
    "                                                   optimizer,\n",
    "                                                   loss_fn,\n",
    "                                                   train_loader,\n",
    "                                                   val_loader,\n",
    "                                                   device,\n",
    "                                                   print_every)\n",
    "        # Validate fucks up\n",
    "        val_loss = validate(model, loss_fn, val_loader, device)\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "              f\"Train loss: {sum(train_loss)/len(train_loss):.3f}, \"\n",
    "              f\"Val. loss: {val_loss:.3f}, \")\n",
    "        train_losses.extend(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        file_name = 'loss_color_seg.txt'\n",
    "        with open(file_name, \"a\") as file:\n",
    "            file.write(f\"Train loss epoch: {sum(train_loss)/len(train_loss):.3f}, Val. loss epoch: {val_loss:.3f}\\n\")\n",
    "        \n",
    "        # Save model to disk after each epoch\n",
    "        torch.save(model.state_dict(), 'segmentation_and_color_model.pth')\n",
    "        \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "def train_epoch(model, optimizer, loss_fn, train_loader, val_loader, device, print_every):\n",
    "    # Train:\n",
    "    model.train()\n",
    "    train_loss_batches = []\n",
    "    num_batches = len(train_loader)\n",
    "    for batch_index, (x, y) in enumerate(train_loader, 1):    \n",
    "        inputs, labels = x.to(device), y.to(device)\n",
    "        inputs = inputs[None, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        z = model.forward(inputs)\n",
    "        loss = loss_fn(z, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_batches.append(loss.item())\n",
    "\n",
    "        print(f\"\\tBatch {batch_index}/{num_batches}\")\n",
    "        # If you want to print your progress more often than every epoch you can\n",
    "        # set `print_every` to the number of batches you want between every status update.\n",
    "        # Note that the print out will trigger a full validation on the full val. set => slows down training\n",
    "        if print_every is not None and batch_index % print_every == 0:\n",
    "            val_loss = validate(model, loss_fn, val_loader, device)\n",
    "            model.train()\n",
    "            print(f\"\\tTrain loss: {sum(train_loss_batches[-print_every:])/print_every:.3f}, \"\n",
    "                  f\"\\tVal. loss: {val_loss:.3f}, \")\n",
    "\n",
    "            file_name = 'loss_color_seg.txt'\n",
    "            with open(file_name, \"a\") as file:\n",
    "                file.write(f\"Train loss batch: {sum(train_loss_batches[-print_every:])/print_every:.3f}, Val. loss batch: {val_loss:.3f}\\n\")\n",
    "    \n",
    "    return model, train_loss_batches\n",
    "\n",
    "def validate(model, loss_fn, val_loader, device):\n",
    "    val_loss_cum = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (x, y) in enumerate(val_loader, 1):\n",
    "            inputs, labels = x.to(device), y.to(device)\n",
    "            inputs = inputs[None, :]\n",
    "            \n",
    "            z = model.forward(inputs)\n",
    "            z = torch.round(z)\n",
    "            #batch_loss = loss_fn(z, labels.long()) \n",
    "            batch_loss = loss_fn(z, labels) \n",
    "            val_loss_cum += batch_loss.item()      \n",
    "            \n",
    "            # Visual test\n",
    "            if batch_index == 1:          \n",
    "                A_output = z[0,0,:,:].to(device)\n",
    "                B_output = z[0,1,:,:].to(device)\n",
    "\n",
    "                inputs = torch.squeeze(inputs)\n",
    "                \n",
    "                L_true = inputs[0,0,:,:] # [128, 128] [img, img]\n",
    "                A_true = labels[0,0,:,:] # [128, 128] [img, img]\n",
    "                B_true = labels[0,1,:,:] # [128, 128] [img, img]\n",
    "\n",
    "                output_img = kornia.color.lab_to_rgb(torch.stack((L_true, A_output, B_output),0)).permute(1, 2, 0)\n",
    "                target_img = kornia.color.lab_to_rgb(torch.stack((L_true, A_true, B_true),0)).permute(1, 2, 0)\n",
    "                \n",
    "                f, axarr = plt.subplots(1,4)\n",
    "                axarr[0].imshow(L_true,'grey')         # L input\n",
    "                axarr[1].imshow(inputs[0,1,:,:])     # SEG input\n",
    "                axarr[2].imshow(output_img) # Output picture (M, N, 3): an image with RGB values (0-1 float or 0-255 int).\n",
    "                axarr[3].imshow(target_img) # Facit picture\n",
    "\n",
    "                plt.show()\n",
    "                # Save the model each time we plot (REMOVE LATER)\n",
    "                torch.save(model.state_dict(), 'segmentation_and_color_model.pth')\n",
    "\n",
    "    return val_loss_cum/len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8731c636-3cf2-46aa-8007-4e33b90e5498",
   "metadata": {},
   "source": [
    "Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b5a346-11e5-4d02-99d2-59f2801da14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_from_disk = True\n",
    "\n",
    "if train_from_disk:  \n",
    "    # Read model from disk\n",
    "    print(\"Reading model from disk\")\n",
    "    pretrained = torch.load('segmentation_and_color_model.pth', map_location=lambda storage, loc: storage)\n",
    "    Segmentaion_and_color_model = build_unet()\n",
    "    Segmentaion_and_color_model.load_state_dict(torch.load('segmentation_and_color_model.pth'))\n",
    "    print(\"Continue writing to loss textfile\")\n",
    "    file_name = 'loss_color_seg.txt'\n",
    "    with open(file_name, \"a\") as file:\n",
    "        file.write('Continue:\\n')\n",
    "else:\n",
    "    # Create new model\n",
    "    print(\"Creating new model\")\n",
    "    Segmentaion_and_color_model = build_unet()\n",
    "    print(\"Starting over writing to loss textfile\")\n",
    "    file_name = 'loss_color_seg.txt'\n",
    "    with open(file_name, \"w\") as file:\n",
    "        file.write('Losses:\\n')\n",
    "\n",
    "optimizer = optim.Adam(Segmentaion_and_color_model.parameters(), lr =0.001)\n",
    "loss_fn = nn.MSELoss() # kass som faen men int nu\n",
    "#loss_fn = nn.CrossEntropyLoss() # mindre kass\n",
    "train_loader = train_dataloader \n",
    "val_loader = val_dataloader\n",
    "num_epochs = 10\n",
    "print_every = 25\n",
    "\n",
    "Segmentaion_and_color_model, Segmentaion_and_color_train_losses, Segmentaion_and_color_val_losses = training_loop(Segmentaion_and_color_model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
